{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLAS and LAPACK\n",
    "\n",
    "We've seen a bit of dense linear algebra using numpy and scipy.  Now we're going to look under the hood.\n",
    "\n",
    "Regardless of what language you're using, chances are if you're doing numerical linear algebra, you are able to take advantage of libraries of code which implement most common linear algebra routines and factorizations.\n",
    "\n",
    "[Basic Linear Algebra Subprograms (BLAS)](https://www.netlib.org/blas/)\n",
    "\n",
    "[Linear Algebra PACKage (LAPACK)](https://www.netlib.org/lapack/)\n",
    "\n",
    "These libraries are wrapped by scipy, which exposes an interface.\n",
    "\n",
    "## Why should you care?\n",
    "\n",
    "First, you probably shouldn't be writing your own basic linear algebra routines if you can avoid it.\n",
    "1.  It takes time to write them\n",
    "2. Even if you know what you're doing, there's a chance you have a bug\n",
    "3. Performance optimization is involved\n",
    "\n",
    "It is entirely possible to do linear algebra in Python without ever worrying about the libraries under the hood.  However, maybe you are prototyping an algorithm in Python, and then want to write compiled/optimized code in C/fortran.  In this case, it is good to be able to translate what you're doing into BLAS/LAPACK routines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Your Configuration\n",
    "\n",
    "You can view what BLAS and LAPACK libraries NumPy is using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "openblas64__info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "blas_ilp64_opt_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "openblas64__lapack_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "lapack_ilp64_opt_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "Supported SIMD extensions in this NumPy install:\n",
      "    baseline = SSE,SSE2,SSE3\n",
      "    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2\n",
      "    not found = AVX512F,AVX512CD,AVX512_KNL,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "\n",
    "np.__config__.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above show the libraries `mkl_rt`, indicating that the system is using Intel's math kernel library (MKL) - this is a library of mathematical functions (including BLAS and LAPACK) which is optimized for Intel CPUs, and is the [default for Anaconda Python](https://docs.continuum.io/mkl-optimizations/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same for scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lapack_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_lapack_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/arm64-builds/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "    runtime_library_dirs = ['/opt/arm64-builds/lib']\n",
      "lapack_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/arm64-builds/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "    runtime_library_dirs = ['/opt/arm64-builds/lib']\n",
      "blas_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "blis_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/arm64-builds/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "    runtime_library_dirs = ['/opt/arm64-builds/lib']\n",
      "blas_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/opt/arm64-builds/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "    runtime_library_dirs = ['/opt/arm64-builds/lib']\n"
     ]
    }
   ],
   "source": [
    "sp.__config__.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS Routines\n",
    "\n",
    "[scipy BLAS interface](https://docs.scipy.org/doc/scipy/reference/linalg.blas.html)\n",
    "\n",
    "BLAS implements *basic* linear algebra routines like dot product, matrix-vector product, and matrix-matrix product as well as triangular solves.\n",
    "\n",
    "It is written in Fortran, so will be easiest to use if you set the flag `order='F'` when constructing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.linalg import blas\n",
    "\n",
    "n = 4096\n",
    "A = np.array(np.random.randn(n,n), order='F')\n",
    "B = np.array(np.random.randn(n,n), order='F')\n",
    "\n",
    "\n",
    "C_np = A @ B # numpy  C = A * B\n",
    "C_blas = blas.dgemm(1.0, A, B) # BLAS C = A * B\n",
    "np.linalg.norm(C_np - C_blas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 s ± 567 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit C_np = np.matmul(A,B) # numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.01 s ± 245 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit C_blas = blas.dgemm(1.0, A, B) # blas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLAS Naming Conventions\n",
    "\n",
    "BLAS functions have short names (like `dgemm`) that look a little cryptic at first.  There is a pattern to the names though.  See [here](https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-fortran/top/blas-and-sparse-blas-routines/blas-routines/naming-conventions-for-blas-routines.html) for reference.\n",
    "\n",
    "```\n",
    "blas.<character><name><mod>\n",
    "```\n",
    "\n",
    "The `<character>` field can be\n",
    "1. `s`: single precision float\n",
    "2. `d`: double precision float\n",
    "3. `c`: single precision complex float\n",
    "4. `z`: double precision complex float\n",
    "\n",
    "The `<name>` field indicate either the operation type, or matrix type.\n",
    "\n",
    "The `<mod>` field indicates additional details about the operation.\n",
    "\n",
    "For example, `dgemm = d + ge + mm` has `d` as the `<character>`, `ge` for `<name>` (general matrix), and `mm` for `<mod>` (matrix multiplication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLAS Levels\n",
    "\n",
    "BLAS is split up into 3 conceptual levels\n",
    "1. Level 1 - vector-vector operations\n",
    "2. Level 2 - matrix-vector operations\n",
    "3. Level 3 - matrix-matrix operations\n",
    "\n",
    "Level 3 operations are most efficient, since they can take the most advantage of memory performance optimizations e.g. minimizing cache misses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FLOPS\n",
    "\n",
    "FLOPS are Floating-Point Operations Per Second, and are one way to measure the power of numerical code.  Floating point operations are counted as the number of `+, *, /, -` applied to floating point numbers.\n",
    "* A human is capable of <1 flop\n",
    "* Your CPU is probably capable of several Giga-flops ($10^9$)\n",
    "* A high end GPU will be measured in Tera-flops ($10^{12}$)\n",
    "* Super computers are mostly O(100) Peta-flops ($10^{17}$).  Some are close to an Exa-flop ($10^{18}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS Level 1\n",
    "\n",
    "Some vector-vector operations (insert the appropriate `<character>`)\n",
    "1. [`axpy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.daxpy.html) ($a x + y$)\n",
    "2. [`dot`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.ddot.html) (dot product $x^H x$)\n",
    "3. [`nrm2`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.dnrm2.html) ($\\|x\\|_2$)\n",
    "\n",
    "See [the SciPy BLAS reference](https://docs.scipy.org/doc/scipy/reference/linalg.blas.html#blas-level-1-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "\n",
    "z = blas.daxpy(x, y, a=1.0)\n",
    "xx = blas.ddot(x, x)\n",
    "x2 = blas.dnrm2(x)\n",
    "np.sqrt(xx) - x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measuring BLAS Level 1 flops using <class 'numpy.float32'>\n",
      "time elapsed = 0.0069005489349365234 sec.\n",
      "2.898320e+09 FLOPS\n"
     ]
    }
   ],
   "source": [
    "# measure FLOPs of fdot\n",
    "n = 10_000_000\n",
    "dtype = np.float32\n",
    "x = np.array(np.random.randn(n), dtype=dtype)\n",
    "y = np.array(np.random.randn(n), dtype=dtype)\n",
    "print(\"measuring BLAS Level 1 flops using {}\".format(dtype))\n",
    "t0 = time.time()\n",
    "xy = blas.sdot(x,y)\n",
    "t1 = time.time()\n",
    "print(\"time elapsed = {} sec.\".format(t1 - t0))\n",
    "flops = (2*n) / (t1 - t0) # 2N FLOP for dot - one multiplication and one addition per entry\n",
    "print(\"{:e} FLOPS\".format(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is consistent with a CPU running at ~3 GHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS Level 2\n",
    "\n",
    "Some matrix-vector operations (insert appropriate `<character>`)\n",
    "\n",
    "1. [`gemv`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.dgemv.html#scipy.linalg.blas.dgemv) $\\alpha A x$\n",
    "2. [`trsv`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.dtrsv.html#scipy.linalg.blas.dtrsv) $L^{-1} x$ (triangular solve)\n",
    "3. [`trmv`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.strmv.html#scipy.linalg.blas.strmv) $L x$ (triangular matrix-vector product)\n",
    "\n",
    "See [the SciPy BLAS reference](https://docs.scipy.org/doc/scipy/reference/linalg.blas.html#blas-level-2-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measuring BLAS Level 2 flops using <class 'numpy.float32'>\n",
      "time elapsed = 0.2551143169403076 sec.\n",
      "1.315270e+08 FLOPS\n"
     ]
    }
   ],
   "source": [
    "# measure FLOPs of gemv\n",
    "n = 4096\n",
    "dtype = np.float32\n",
    "A = np.array(np.random.randn(n, n), dtype=dtype)\n",
    "x = np.array(np.random.randn(n), dtype=dtype)\n",
    "print(\"measuring BLAS Level 2 flops using {}\".format(dtype))\n",
    "t0 = time.time()\n",
    "y = blas.sgemv(1.0, A, x)\n",
    "t1 = time.time()\n",
    "print(\"time elapsed = {} sec.\".format(t1 - t0))\n",
    "flops = (2 * n**2) / (t1 - t0) # 2 n**2 FLOP - one multiplication and one addition per entry of A\n",
    "print(\"{:e} FLOPS\".format(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS Level 3\n",
    "\n",
    "Some matrix-matrix operations (insert appropriate `<character>`)\n",
    "\n",
    "1. [`gemm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.dgemm.html#scipy.linalg.blas.dgemm) $\\alpha AB$\n",
    "2. [`syrk`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.dsyrk.html#scipy.linalg.blas.dsyrk) $\\alpha A A^T$\n",
    "3. [`trmm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.blas.dtrmm.html#scipy.linalg.blas.dtrmm) $\\alpha L_1 L_2$ (triangular multiplication)\n",
    "\n",
    "See the [SciPy BLAS reference](https://docs.scipy.org/doc/scipy/reference/linalg.blas.html#blas-level-3-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measuring BLAS Level 3 flops using <class 'numpy.float32'>\n",
      "time elapsed = 1.4467544555664062 sec.\n",
      "9.499812e+10 FLOPS\n"
     ]
    }
   ],
   "source": [
    "# measure FLOPs of gemm\n",
    "n = 4096\n",
    "dtype = np.float32\n",
    "A = np.array(np.random.randn(n, n), dtype=dtype)\n",
    "B = np.array(np.random.randn(n, n), dtype=dtype)\n",
    "print(\"measuring BLAS Level 3 flops using {}\".format(dtype))\n",
    "t0 = time.time()\n",
    "C = blas.sgemm(1.0, A, B)\n",
    "t1 = time.time()\n",
    "print(\"time elapsed = {} sec.\".format(t1 - t0))\n",
    "flops = (2 * n**3) / (t1 - t0) # 2 n**3 FLOP - n multiplications and additions per entry of C\n",
    "print(\"{:e} FLOPS\".format(flops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our experminets, we see that the BLAS level 3 operation gives us the most FLOPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAPACK Routines\n",
    "\n",
    "[scipy LAPACK interface](https://docs.scipy.org/doc/scipy/reference/linalg.lapack.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import lapack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAPACK is a library of linear algebra routines that go beyond basic operations.  These include routines for various factorizations and eigenvalue and singular value decompositions.\n",
    "\n",
    "Again, the names are a bit cryptic, and it is worth searching online (and reading documentation) to figure out how to call the right functions.\n",
    "\n",
    "### Example: Obtain a orthonormal basis for columns of a matrix\n",
    "\n",
    "In a variety of situations it is convenient to be able to obtain an orthonormal basis for columns of a matrix `A` (e.g. subspace iteration, randomized numerical linear algebra).  One way to do this is to use the QR decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "3.8333020695109416e-15\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "n = 100\n",
    "A = np.array(np.random.randn(m, n), order='F')\n",
    "\n",
    "def get_Q_qr(A):\n",
    "    Q, R = la.qr(A, mode='economic')\n",
    "    return Q\n",
    "\n",
    "Q = get_Q_qr(A)\n",
    "\n",
    "print(Q.shape)\n",
    "print(np.linalg.norm(Q.T @ Q - np.eye(n))) # measure how close Q is to orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get this using LAPACK routines as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "4.137291081848141e-15\n"
     ]
    }
   ],
   "source": [
    "def get_Q_lapack(A):\n",
    "    qr, tau, work, info = lapack.dgeqrf(A)\n",
    "    Q, work, info = lapack.dorgqr(qr, tau)\n",
    "    return Q\n",
    "\n",
    "A = np.array(np.random.randn(m, n), order='F')\n",
    "Q = get_Q_lapack(A)\n",
    "\n",
    "print(Q.shape)\n",
    "print(np.linalg.norm(Q.T @ Q - np.eye(n))) # measure how close Q is to orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do many LAPACK (and BLAS) routines in-place, meaning you can overwrite the matrix.  This saves time used for memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "3.791123658653987e-15\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def get_Q_inplace(A):\n",
    "    m, n = A.shape\n",
    "    lwork = max(3*n, 1)\n",
    "    qr, tau, work, info = lapack.dgeqrf(A, lwork, 1) # overwrite A = True\n",
    "    Q, work, info = lapack.dorgqr(qr, tau, lwork, 1) # overwrite qr = True\n",
    "    return Q\n",
    "\n",
    "A = np.array(np.random.randn(m, n), order='F')\n",
    "Q = get_Q_inplace(A)\n",
    "\n",
    "print(Q.shape)\n",
    "print(np.linalg.norm(Q.T @ Q - np.eye(n))) # measure how close Q is to orthogonal\n",
    "print(np.linalg.norm(A - Q)) # A now contains Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timing <function get_Q_qr at 0x7f5e69cdbb80>\n",
      "  time elapsed = 0.018201112747192383 sec.\n",
      "timing <function get_Q_lapack at 0x7f5e69b1d3a0>\n",
      "  time elapsed = 0.015674829483032227 sec.\n",
      "timing <function get_Q_inplace at 0x7f5e69b1d1f0>\n",
      "  time elapsed = 0.010359764099121094 sec.\n"
     ]
    }
   ],
   "source": [
    "m = 4096\n",
    "n = 128\n",
    "\n",
    "for get_Q in (get_Q_qr, get_Q_lapack, get_Q_inplace):\n",
    "    \n",
    "    print(\"timing {}\".format(get_Q))\n",
    "    A = np.array(np.random.randn(m, n), order='F')\n",
    "    t0 = time.time()\n",
    "    Q = get_Q(A)\n",
    "    t1 = time.time()\n",
    "    print(\"  time elapsed = {} sec.\".format(t1 - t0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the in-place version is fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "One way to get the top `k`-dimensional eigenspace (eigenpace with k-largest eigenvalues) is using a subspace version of the power method.  Here is some pseudocode:\n",
    "```\n",
    "\n",
    "# get top k-dimensional eigenspace of symmetric matrix A\n",
    "m, n = A.shape # m should equal n\n",
    "Q = random n x k matrix\n",
    "Q, R = qr(Q) # orthogonalize\n",
    "for some number of iterations:\n",
    "    Q = A @ Q\n",
    "    Q, R = qr(Q) # re-othogonalize\n",
    "    \n",
    "return Q\n",
    "```\n",
    "\n",
    "Implement a function that realizes this algorithm.  Use BLAS to implement matrix-matrix multiplication, and use LAPACK to do the orthogonalization of Q.\n",
    "\n",
    "Compare this to the span of the top-k eigenvectors of `A` obtained via `eigh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement a version of `solve` for a square matrix `A` which uses LAPACK for the LU decomposition, and BLAS for the triangular solves.  You may want to look at [dgetrf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lapack.dgetrf.html#scipy.linalg.lapack.dgetrf) (the `lu` return object has L in the lower triangular part, and U in the upper triangular part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
